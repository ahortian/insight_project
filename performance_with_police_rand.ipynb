{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import calendar\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "#from sklearn import linear_model\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn import ensemble\n",
    "from sklearn import linear_model\n",
    "#Import Random Forest Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# scaling the input feature\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "\n",
    "from joblib import load\n",
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('crime_data/fullTrainTestCrimeData_train_weather_by_hourgroup_police_rand_2019_06_27.csv')\n",
    "df_train = df_train.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "#df_train.boxplot(column=\"Value\")\n",
    "# remove outliers\n",
    "ind_to_drop_outlier = df_train[df_train[\"Value\"]>60].index\n",
    "df_train = df_train.drop(ind_to_drop_outlier, axis=0)\n",
    "\n",
    "df_valid = pd.read_csv('crime_data/fullTrainTestCrimeData_valid_weather_by_hourgroup_police_rand_2019_06_27.csv')\n",
    "df_valid = df_valid.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "df_test = pd.read_csv('crime_data/fullTrainTestCrimeData_tests_weather_by_hourgroup_police_rand_2019_06_27.csv')\n",
    "df_test = df_test.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "#df_uniq_id[\"DATE\"] = df_uniq_id[\"OCCURRED_ON_DATE\"].apply(lambda x: x.split(\" \")[0])\n",
    "df_train[\"Value_Occ\"] = df_train[\"Value\"].apply(lambda x: 1 if x>0 else x)\n",
    "df_valid[\"Value_Occ\"] = df_valid[\"Value\"].apply(lambda x: 1 if x>0 else x)\n",
    "df_test[\"Value_Occ\"]  = df_test[\"Value\"].apply(lambda x: 1 if x>0 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_train[[\"Hour_Grp\",\"DAY_OF_MONTH\",\"DAY_OF_WEEK_NUM\",\"MONTH\",\"Lat\",\"Long\"\n",
    "                    ,\"Temperature(F)\",\"Humidity(%)\",\"Wind Speed(mph)\",\"Precip.(in)\",\n",
    "                    \"closest_police_d\",\"closest_mbta_d\"]]\n",
    "\n",
    "x_valid = df_valid[[\"Hour_Grp\",\"DAY_OF_MONTH\",\"DAY_OF_WEEK_NUM\",\"MONTH\",\"Lat\",\"Long\"\n",
    "                    ,\"Temperature(F)\",\"Humidity(%)\",\"Wind Speed(mph)\",\"Precip.(in)\",\n",
    "                    \"closest_police_d\",\"closest_mbta_d\"]]\n",
    "\n",
    "x_test = df_test[[\"Hour_Grp\",\"DAY_OF_MONTH\",\"DAY_OF_WEEK_NUM\",\"MONTH\",\"Lat\",\"Long\"\n",
    "                    ,\"Temperature(F)\",\"Humidity(%)\",\"Wind Speed(mph)\",\"Precip.(in)\",\n",
    "                  \"closest_police_d\",\"closest_mbta_d\"]]\n",
    "\n",
    "y_train = df_train[[\"Value_Occ\"]]\n",
    "y_valid = df_valid[[\"Value_Occ\"]]\n",
    "y_test = df_test[[\"Value_Occ\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at data, how was it built? \n",
    "- See code: segment_map_time_reduce_police_random.ipynb\n",
    "- train is evenly grids \n",
    "- valid and test are random around the train grids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MEAN\n",
    "### Calculate Mean Values\n",
    "- for a location, calculate the average over time\n",
    "- this is calculated on the training sample only\n",
    "    - think of it as a model which is trained on training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 24\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import calendar\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# parameters setup for segmenting the area\n",
    "grid_w = 0.5 # mi\n",
    "grid_deg = round((grid_w/111)*1.60934, 4)\n",
    "\n",
    "lat_dn = 42.2324\n",
    "lat_up = 42.3951\n",
    "lng_dn = -71.1787\n",
    "lng_up = -70.9636\n",
    "\n",
    "# start_day = datetime.datetime(2015,6,15)\n",
    "# last_day = datetime.datetime(2015,9,30)\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "\n",
    "fp = \"/Users/apicharthortiangtham/InsightDS/1_work/2019_06_07_boston_crime_project/bos_land/bos_land.shp\"\n",
    "data_shapemap = gpd.read_file(fp)\n",
    "\n",
    "# Convert data to global lat long\n",
    "# EPSG 4326 = WGS84 = https://epsg.io/4326\n",
    "data_shapemap_geo = data_shapemap.to_crs(epsg=4326) \n",
    "multi_polygon = list(data_shapemap_geo[\"geometry\"][0])\n",
    "\n",
    "def in_MutiPolygon(lat, lng, multi_polygon):\n",
    "    is_inside = False\n",
    "    for polg in multi_polygon:\n",
    "        point = Point(lng, lat)\n",
    "        is_inside = polg.contains(point)\n",
    "        if is_inside:\n",
    "            return is_inside\n",
    "    return is_inside\n",
    "\n",
    "a = lat_dn\n",
    "lat_list=[]\n",
    "lat_list+=[round(a,4)]\n",
    "while a <= lat_up:\n",
    "    a+= grid_deg\n",
    "    lat_list += [round(a,4)] \n",
    "    \n",
    "b = lng_dn\n",
    "lng_list=[]\n",
    "lng_list+=[round(b,4)]\n",
    "while b <= lng_up:\n",
    "    b+= grid_deg\n",
    "    lng_list += [round(b,4)]\n",
    "print (len(lng_list),len(lat_list))\n",
    "\n",
    "hour_groups = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count :  1\n",
      "count :  2\n",
      "count :  3\n",
      "count :  4\n",
      "count :  5\n",
      "count :  6\n",
      "count :  7\n",
      "count :  8\n",
      "count :  9\n",
      "count :  10\n",
      "count :  11\n",
      "count :  12\n",
      "count :  13\n",
      "count :  14\n",
      "count :  15\n",
      "count :  16\n",
      "count :  17\n",
      "count :  18\n",
      "count :  19\n",
      "count :  20\n",
      "count :  21\n",
      "count :  22\n",
      "count :  23\n",
      "count :  24\n",
      "count :  25\n",
      "count :  26\n",
      "count :  27\n",
      "count :  28\n",
      "count :  29\n",
      "count :  30\n",
      "count :  31\n"
     ]
    }
   ],
   "source": [
    "#def create_mean_feature(df_train):\n",
    "cnt = 0\n",
    "data = []\n",
    "for lng in lng_list:\n",
    "    cnt +=1 \n",
    "    print (\"count : \", cnt )\n",
    "    for lat in lat_list:\n",
    "        in_Bos = in_MutiPolygon(lat, lng, multi_polygon)\n",
    "        if in_Bos:\n",
    "            for this_hr in hour_groups:\n",
    "                con1 = (df_train[\"Lat\"] < lat + 0.000001)\n",
    "                con2 = (df_train[\"Lat\"] > lat - 0.000001)\n",
    "                con3 = (df_train[\"Long\"] < lng + 0.000001)\n",
    "                con4 = (df_train[\"Long\"] > lng - 0.000001)\n",
    "                \n",
    "                _mean = df_train[ con1&con2&con3&con4 & (df_train[\"Hour_Grp\"] == this_hr)][\"Value_Occ\"].mean()\n",
    "                #print (cnt, lng, lat,  _mean)\n",
    "                dat = { 'Lat': lat,'Long': lng, \n",
    "                        'Lat_int': int(lat*1000),'Long_int': int(lng*1000), \n",
    "                           'Hour_Grp':this_hr,\n",
    "                           'mean': round(_mean),\n",
    "                           'mean_raw': _mean,\n",
    "                          }\n",
    "                data += [dat]\n",
    "train_mean = pd.DataFrame(data)\n",
    "#    return train_mean\n",
    "#train_mean = create_mean_feature(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MEAN prediction for the Test sample : real calculation \n",
    "   - since now the validate and test sample have random grid poits\n",
    "   - we need more caluclation to get mean of these sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_predict(inp_lat, inp_lng, inp_hr_gr, grid_deg, df_mean_in): \n",
    "    #tmp_df[\"Lat\"].iloc[0]\n",
    "    #tmp_df[\"Lat\"]\n",
    "    cond2 = df_mean_in[\"Lat\"] < (inp_lat + 1.1*grid_deg)\n",
    "    cond3 = df_mean_in[\"Lat\"] > (inp_lat - 1.1*grid_deg)\n",
    "    cond4 = df_mean_in[\"Long\"] < (inp_lng + 1.1*grid_deg)\n",
    "    cond5 = df_mean_in[\"Long\"] > (inp_lng - 1.1*grid_deg)\n",
    "    cond6 = df_mean_in[\"Hour_Grp\"] == inp_hr_gr\n",
    "    tmp_df = df_mean_in[cond2 & cond3 & cond4 & cond5 & cond6]\n",
    "    #print (inp_lat, inp_lng)\n",
    "    #print (tmp_df)\n",
    "    \n",
    "    min_dist = 200\n",
    "    dist = 0\n",
    "    y_dist = 0\n",
    "    x_dist = 0\n",
    "    mean_val = 0\n",
    "    \n",
    "    for i in range(tmp_df.shape[0]):\n",
    "        la = tmp_df[\"Lat\"].iloc[i]\n",
    "        ln = tmp_df[\"Long\"].iloc[i]\n",
    "        \n",
    "        y_dist = abs(inp_lat-la) *(111/1.60934)\n",
    "        x_dist = abs(inp_lng-ln) *(111/1.60934)\n",
    "        dist = math.sqrt((y_dist**2) + (x_dist**2))\n",
    "        \n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            mean_val = tmp_df[\"mean\"].iloc[i]\n",
    "            #print (\"min_dist \", min_dist, la, ln, mean_val)\n",
    "    \n",
    "    return mean_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "Elapsed time:  421.33611488342285\n"
     ]
    }
   ],
   "source": [
    "print (\"Warning: This takes 8 minutes to finish\")\n",
    "start_time = time.time()\n",
    "#df_test[\"mean\"] = 0\n",
    "test_sample_meanout = []\n",
    "for i in range(df_test.shape[0]):\n",
    "#for i in range(df_test[:500].shape[0]):\n",
    "    if i%10000 == 0: print (i)\n",
    "    _mean = get_mean_predict(df_test[\"Lat\"].iloc[i], df_test[\"Long\"].iloc[i], df_test[\"Hour_Grp\"].iloc[i], grid_deg, train_mean)\n",
    "    test_sample_meanout += [_mean]\n",
    "    #print (\"_mean \", _mean)\n",
    "print (\"Elapsed time: \", str(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_sample_meanout' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b8c75faa7350>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_test_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'test_set_cal_mean'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_sample_meanout\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#print (y_test_mean.head())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_test_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Value_Occ\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0;34m\"MEAN f1_score      : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0;34m\"MEAN accuracy_score: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_sample_meanout' is not defined"
     ]
    }
   ],
   "source": [
    "y_test_mean = pd.DataFrame({'test_set_cal_mean': test_sample_meanout})\n",
    "#print (y_test_mean.head())\n",
    "print ( \"MEAN f1_score      : \", f1_score(y_test, y_test_mean))\n",
    "print ( \"MEAN accuracy_score: \", accuracy_score(y_test, y_test_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results of calculation for later use\n",
    "#y_test_mean.to_csv('crime_data/test_cal_mean_police_rand_2019_06_27.csv')\n",
    "y_test_mean.to_csv('crime_data/test_cal_mean_police_rand_xxxxx.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read from saved results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_mean = pd.read_csv(\"crime_data/test_cal_mean_police_rand_2019_06_27.csv\")\n",
    "y_test_mean = y_test_mean.drop(columns=[\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN f1_score      :  0.5527150217994452\n",
      "MEAN accuracy_score:  0.8082738702004757\n"
     ]
    }
   ],
   "source": [
    "y_test_true = df_test[[\"Value_Occ\"]]\n",
    "print ( \"MEAN f1_score      : \", f1_score(y_test, y_test_mean))\n",
    "print ( \"MEAN accuracy_score: \", accuracy_score(y_test, y_test_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report MEAN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.91      0.88    125556\n",
      "           1       0.62      0.50      0.55     39252\n",
      "\n",
      "    accuracy                           0.81    164808\n",
      "   macro avg       0.74      0.70      0.72    164808\n",
      "weighted avg       0.80      0.81      0.80    164808\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Report MEAN\")\n",
    "print(classification_report(y_test, y_test_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grad Boosted DT\n",
    "- #### Load model \n",
    "- #### or Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/insight/lib/python3.7/site-packages/sklearn/ensemble/gradient_boosting.py:1450: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('preprocessor',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='passthrough',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('somename',\n",
       "                                                  StandardScaler(copy=True,\n",
       "                                                                 with_mean=True,\n",
       "                                                                 with_std=True),\n",
       "                                                  ['Lat', 'Long',\n",
       "                                                   'Temperature(F)',\n",
       "                                                   'Humidity(%)',\n",
       "                                                   'Wind Speed(mph)',\n",
       "                                                   'Precip.(in)',\n",
       "                                                   'closest_police_d',\n",
       "                                                   'closest_mbta_d'])],\n",
       "                                   verbose=Fa...\n",
       "                                            learning_rate=0.2, loss='deviance',\n",
       "                                            max_depth=6, max_features=None,\n",
       "                                            max_leaf_nodes=None,\n",
       "                                            min_impurity_decrease=0.0,\n",
       "                                            min_impurity_split=None,\n",
       "                                            min_samples_leaf=1,\n",
       "                                            min_samples_split=2,\n",
       "                                            min_weight_fraction_leaf=0.0,\n",
       "                                            n_estimators=100,\n",
       "                                            n_iter_no_change=None,\n",
       "                                            presort='auto', random_state=42,\n",
       "                                            subsample=1.0, tol=0.0001,\n",
       "                                            validation_fraction=0.1, verbose=0,\n",
       "                                            warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.2,\n",
    "    'criterion': 'friedman_mse',\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "        ('somename', StandardScaler(), \n",
    "         [\"Lat\",\"Long\",\"Temperature(F)\",\"Humidity(%)\",\"Wind Speed(mph)\",\"Precip.(in)\",\"closest_police_d\",\"closest_mbta_d\"])\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "grad_boost_classi_pipe = Pipeline(steps=[('preprocessor', ct),\n",
    "                      ('classifier', ensemble.GradientBoostingClassifier(**params))])\n",
    "\n",
    "grad_boost_classi_pipe.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test f1  0.5277190700699074\n",
      "test accuracy_score  0.8061077132178049\n"
     ]
    }
   ],
   "source": [
    "predictions_test = grad_boost_classi_pipe.predict(x_test)\n",
    "print (\"test f1 \", f1_score(y_test, predictions_test))\n",
    "print (\"test accuracy_score \", accuracy_score(y_test, predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validate f1  0.5798109482657183\n",
      "validate accuracy_score  0.8044502044392523\n"
     ]
    }
   ],
   "source": [
    "predictions_valid = grad_boost_classi_pipe.predict(x_valid)\n",
    "print (\"validate f1 \", f1_score(y_valid, predictions_valid))\n",
    "print (\"validate accuracy_score \", accuracy_score(y_valid, predictions_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train f1  0.5787184674965111\n",
      "train accuracy_score  0.8090214789859601\n"
     ]
    }
   ],
   "source": [
    "predictions_train = grad_boost_classi_pipe.predict(x_train)\n",
    "print (\"train f1 \", f1_score(y_train, predictions_train))\n",
    "print (\"train accuracy_score \", accuracy_score(y_train, predictions_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grad_boost_classi_pipe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-aa4e4104aeee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# https://www.mikulskibartosz.name/how-to-save-a-machine-learning-model-into-a-file/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m toBePersisted = dict({\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;34m'model'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgrad_boost_classi_pipe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     'metadata': {\n\u001b[1;32m      6\u001b[0m         \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Does a crime occur?'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'grad_boost_classi_pipe' is not defined"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "# https://www.mikulskibartosz.name/how-to-save-a-machine-learning-model-into-a-file/\n",
    "toBePersisted = dict({\n",
    "    'model': grad_boost_classi_pipe,\n",
    "    'metadata': {\n",
    "        'name': 'Does a crime occur?',\n",
    "        'author': 'Apichart H',\n",
    "        \n",
    "    }\n",
    "})\n",
    "\n",
    "dump(toBePersisted, 'grad_bdt_classi_police_mbta_xxxxxxx.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
